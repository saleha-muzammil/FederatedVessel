{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import random_split, DataLoader\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [5, 5]","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = MNIST('/kaggle/working', train=True, download=True, transform=transforms.ToTensor())\ntest_dataset = MNIST('/kaggle/working', train=False, download=True, transform=transforms.ToTensor())\n\ntrain_dataset, dev_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.83), int(len(train_dataset) * 0.17)])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Notebook Constants","metadata":{}},{"cell_type":"code","source":"total_train_size = len(train_dataset)\ntotal_test_size = len(test_dataset)\ntotal_dev_size = len(dev_dataset)\n\nclasses = 10\ninput_dim = 784\n\nnum_clients = 8\nrounds = 30\nbatch_size = 128\nepochs_per_client = 3\nlearning_rate = 2e-2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train_size, total_dev_size, total_test_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define utilities for GPU support","metadata":{}},{"cell_type":"code","source":"def get_device():\n    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader(DataLoader):\n        def __init__(self, dl, device):\n            self.dl = dl\n            self.device = device\n\n        def __iter__(self):\n            for batch in self.dl:\n                yield to_device(batch, self.device)\n\n        def __len__(self):\n            return len(self.dl)\n\ndevice = get_device()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define FederatedNet class","metadata":{}},{"cell_type":"code","source":"class FederatedNet(torch.nn.Module):    \n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 20, 7)\n        self.conv2 = torch.nn.Conv2d(20, 40, 7)\n        self.maxpool = torch.nn.MaxPool2d(2, 2)\n        self.flatten = torch.nn.Flatten()\n        self.linear = torch.nn.Linear(2560, 10)\n        self.non_linearity = torch.nn.functional.relu\n        self.track_layers = {'conv1': self.conv1, 'conv2': self.conv2, 'linear': self.linear}\n    \n    def forward(self, x_batch):\n        out = self.conv1(x_batch)\n        out = self.non_linearity(out)\n        out = self.conv2(out)\n        out = self.non_linearity(out)\n        out = self.maxpool(out)\n        out = self.flatten(out)\n        out = self.linear(out)\n        return out\n    \n    def get_track_layers(self):\n        return self.track_layers\n    \n    def apply_parameters(self, parameters_dict):\n        with torch.no_grad():\n            for layer_name in parameters_dict:\n                self.track_layers[layer_name].weight.data *= 0\n                self.track_layers[layer_name].bias.data *= 0\n                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n    \n    def get_parameters(self):\n        parameters_dict = dict()\n        for layer_name in self.track_layers:\n            parameters_dict[layer_name] = {\n                'weight': self.track_layers[layer_name].weight.data, \n                'bias': self.track_layers[layer_name].bias.data\n            }\n        return parameters_dict\n    \n    def batch_accuracy(self, outputs, labels):\n        with torch.no_grad():\n            _, predictions = torch.max(outputs, dim=1)\n            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n    \n    def _process_batch(self, batch):\n        images, labels = batch\n        outputs = self(images)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        accuracy = self.batch_accuracy(outputs, labels)\n        return (loss, accuracy)\n    \n    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n        optimizer = opt(self.parameters(), lr)\n        history = []\n        for epoch in range(epochs):\n            losses = []\n            accs = []\n            for batch in dataloader:\n                loss, acc = self._process_batch(batch)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                loss.detach()\n                losses.append(loss)\n                accs.append(acc)\n            avg_loss = torch.stack(losses).mean().item()\n            avg_acc = torch.stack(accs).mean().item()\n            history.append((avg_loss, avg_acc))\n        return history\n    \n    def evaluate(self, dataset, batch_size=128):\n        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n        losses = []\n        accs = []\n        with torch.no_grad():\n            for batch in dataloader:\n                loss, acc = self._process_batch(batch)\n                losses.append(loss)\n                accs.append(acc)\n        avg_loss = torch.stack(losses).mean().item()\n        avg_acc = torch.stack(accs).mean().item()\n        return (avg_loss, avg_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Client class","metadata":{}},{"cell_type":"code","source":"class Client:\n    def __init__(self, client_id, dataset):\n        self.client_id = client_id\n        self.dataset = dataset\n    \n    def get_dataset_size(self):\n        return len(self.dataset)\n    \n    def get_client_id(self):\n        return self.client_id\n    \n    def train(self, parameters_dict):\n        net = to_device(FederatedNet(), device)\n        net.apply_parameters(parameters_dict)\n        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n        return net.get_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup clients ","metadata":{}},{"cell_type":"code","source":"examples_per_client = total_train_size // num_clients\nclient_datasets = random_split(train_dataset, [min(i + examples_per_client, \n           total_train_size) - i for i in range(0, total_train_size, examples_per_client)])\nclients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Start server","metadata":{}},{"cell_type":"code","source":"global_net = to_device(FederatedNet(), device)\nhistory = []\nfor i in range(rounds):\n    print('Start Round {} ...'.format(i + 1))\n    curr_parameters = global_net.get_parameters()\n    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n    for client in clients:\n        client_parameters = client.train(curr_parameters)\n        fraction = client.get_dataset_size() / total_train_size\n        for layer_name in client_parameters:\n            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n    global_net.apply_parameters(new_parameters)\n    \n    train_loss, train_acc = global_net.evaluate(train_dataset)\n    dev_loss, dev_acc = global_net.evaluate(dev_dataset)\n    print('After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n            round(dev_loss, 4), round(dev_acc, 4)))\n    history.append((train_loss, dev_loss))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([i + 1 for i in range(len(history))], [history[i][0] for i in range(len(history))], color='r', label='train loss')\nplt.plot([i + 1 for i in range(len(history))], [history[i][1] for i in range(len(history))], color='b', label='dev loss')\nplt.legend()\nplt.title('Training history')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}